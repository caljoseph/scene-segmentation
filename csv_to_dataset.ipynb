{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "BwkS2rMIUtS0",
        "outputId": "46f4ca9a-c22b-4d53-be3b-b5645de8e5fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-2f838ee0-b369-4fad-9737-a3283685f467\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-2f838ee0-b369-4fad-9737-a3283685f467\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving A_Christmas_Carol.xlsx to A_Christmas_Carol.xlsx\n",
            "Saving Collision_of_Worlds_1.xlsx to Collision_of_Worlds_1.xlsx\n",
            "Saving Collision_of_Worlds_2.xlsx to Collision_of_Worlds_2.xlsx\n",
            "Saving Collision_of_Worlds_3.xlsx to Collision_of_Worlds_3.xlsx\n",
            "Saving Collision_of_Worlds_4.xlsx to Collision_of_Worlds_4.xlsx\n",
            "Saving Collision_of_Worlds_5.xlsx to Collision_of_Worlds_5.xlsx\n",
            "Saving Falling_Sentences.xlsx to Falling_Sentences.xlsx\n",
            "Saving Frankenstein.xlsx to Frankenstein.xlsx\n",
            "Saving In_Amundsen’s_Tent.xlsx to In_Amundsen’s_Tent.xlsx\n",
            "Saving Leiningen_Versus_the_Ants.xlsx to Leiningen_Versus_the_Ants.xlsx\n",
            "Saving Observer_1-A_Warm_Home.xlsx to Observer_1-A_Warm_Home.xlsx\n",
            "Saving Observer_2-Charity.xlsx to Observer_2-Charity.xlsx\n",
            "Saving Observer_3-One_of_Us.xlsx to Observer_3-One_of_Us.xlsx\n",
            "Saving Observer_4-Legends.xlsx to Observer_4-Legends.xlsx\n",
            "Saving Prince_and_the_Pauper.xlsx to Prince_and_the_Pauper.xlsx\n",
            "Saving The_Demon_King.xlsx to The_Demon_King.xlsx\n",
            "Saving The_Exiled_Queen.xlsx to The_Exiled_Queen.xlsx\n",
            "Saving The_Human_Chair.xlsx to The_Human_Chair.xlsx\n",
            "Saving The_Landlady.xlsx to The_Landlady.xlsx\n",
            "Saving The_Legend_of_Sleepy_Hollow.xlsx to The_Legend_of_Sleepy_Hollow.xlsx\n",
            "Saving The_Man_In_The_Well.xlsx to The_Man_In_The_Well.xlsx\n",
            "Saving The_Monkey_s_Paw.xlsx to The_Monkey_s_Paw.xlsx\n",
            "Saving The_Most_Dangerous_Game.xlsx to The_Most_Dangerous_Game.xlsx\n",
            "Saving The_Necklace.xlsx to The_Necklace.xlsx\n",
            "Saving The_Night_Rider.xlsx to The_Night_Rider.xlsx\n",
            "Saving The_Night_Wire.xlsx to The_Night_Wire.xlsx\n",
            "Saving The_Old_Man_And_The_Sea.xlsx to The_Old_Man_And_The_Sea.xlsx\n",
            "Saving The_Ransom_of_Red_Chief.xlsx to The_Ransom_of_Red_Chief.xlsx\n",
            "Saving The_Sun_Also_Rises.xlsx to The_Sun_Also_Rises.xlsx\n",
            "Saving There_Will_Come_Soft_Rains.xlsx to There_Will_Come_Soft_Rains.xlsx\n",
            "Saving To_Build_A_Fire.xlsx to To_Build_A_Fire.xlsx\n",
            "Saving Uncle_Toms_Cabin.xlsx to Uncle_Toms_Cabin.xlsx\n",
            "Saving Winnie_The_Pooh.xlsx to Winnie_The_Pooh.xlsx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stories = {}\n",
        "dataset = []\n",
        "\n",
        "MEAN = 2  # Mean number of sentences radius\n",
        "STD_DV = 2  # Standard deviation size\n",
        "MIN_RADIUS = 1  # Minimum radius size\n",
        "MAX_RADIUS = 6  # Maximum radius size\n",
        "\n",
        "TOKEN_NEIGHBORHOOD_RADIUS = 30 # These aren't implemnented fully yet\n",
        "TOKEN_LENGTH = 4\n",
        "\n",
        "for file_name, file_data in uploaded.items():\n",
        "    try:\n",
        "        df = pd.read_excel(file_data, header=None)\n",
        "\n",
        "        # Check for annotation\n",
        "        if 1 not in df[0].values:\n",
        "          print(f\"No scene transition found in {file_name}, skipping...\")\n",
        "          continue\n",
        "\n",
        "        # Find the index of the first '1' in the first column\n",
        "        first_scene_transition_index = df[df[0] == 1].index[0]\n",
        "\n",
        "        # Replace newlines in the sentences column\n",
        "        df[1] = df[1].astype(str).apply(lambda x: x.replace('\\n', ' '))\n",
        "\n",
        "        # fill any missed entries with 0\n",
        "        df[0] = df[0].fillna(0)\n",
        "\n",
        "         # Remove rows where the sentence is empty\n",
        "        df = df.dropna(subset=[1])\n",
        "        df = df[df[1] != 0]\n",
        "        df = df[df[1] != '0']\n",
        "\n",
        "        # # Slice the DataFrame to keep only the rows from the first '1' onwards\n",
        "        df = df.iloc[first_scene_transition_index:].reset_index(drop=True)\n",
        "\n",
        "        stories[file_name] = df\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading {file_name}: {e}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "e9Dds3HLU4vz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "553a85b5-2321-4b91-e002-e7c9e50caea1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No scene transition found in Prince_and_the_Pauper.xlsx, skipping...\n",
            "No scene transition found in The_Legend_of_Sleepy_Hollow.xlsx, skipping...\n",
            "No scene transition found in There_Will_Come_Soft_Rains.xlsx, skipping...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def print_neighborhood_info(neighborhood, start_idx, end_idx):\n",
        "\n",
        "    print(f\"Neighborhood from index {start_idx} to {end_idx} (Length: {end_idx - start_idx} sentences):\")\n",
        "    for index, row in neighborhood.iterrows():\n",
        "        print(f\"Index: {index}, Label: {row[0]} : {row[1]} \")\n",
        "    print(\"------\\n\")\n",
        "\n",
        "def generate_gaussian_window_size(mean, std_dev, min_size, max_size):\n",
        "    size = round(np.random.normal(mean, std_dev))\n",
        "    return max(min_size, min(size, max_size))\n",
        "\n",
        "def flatten_control(neighborhood):\n",
        "    filtered_sentences = neighborhood[1].dropna().tolist()\n",
        "\n",
        "    # Join the sentences into a single string with spaces\n",
        "    concatenated_sentences = ' '.join(filtered_sentences)\n",
        "\n",
        "    # Create and return DataFrame with a single entry\n",
        "    return pd.DataFrame({'isTransition': [0], 'text': [concatenated_sentences]})"
      ],
      "metadata": {
        "id": "_6PJMbz1EduH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_sentence_neighborhoods(df, mean_radius, std_dev_radius, min_radius, max_radius):\n",
        "\n",
        "    \"\"\"\n",
        "    Generates sentence neighborhoods centered around transition points in a DataFrame.\n",
        "\n",
        "    Parameters:\n",
        "    - df (pandas.DataFrame) : Assumption is that the first row is a scene transition\n",
        "    - radius : Number of sentences around the transition to include (before offsets)\n",
        "\n",
        "    Returns:\n",
        "    - list of pandas.DataFrame: A list containing the neighborhoods as DataFrames with one entry. A flattening of the neighborhood\n",
        "\n",
        "    Notes:\n",
        "    - For adjacent transitions (or story beginnings) only one neighborhood in the direction opposite to the conflict is returned\n",
        "    - Only neighborhoods that contain at least one sentence before or after the transition (not including\n",
        "      the transition itself) are included in the output.\n",
        "    \"\"\"\n",
        "\n",
        "    neighborhoods = []\n",
        "\n",
        "    transition_indices = df.index[df[0] == 1].tolist()  # Scene transition indices\n",
        "\n",
        "    for idx in transition_indices:\n",
        "      radius = generate_gaussian_window_size(mean_radius, std_dev_radius, min_radius, max_radius)\n",
        "      length = ( radius * 2 ) + 1\n",
        "\n",
        "      # These are the number of rows I can crawl up and down without hitting another transition\n",
        "      max_prev_distance = get_distance_to_prev_transition(df, idx) - 1\n",
        "      max_next_distance = get_distance_to_next_transition(df, idx) - 1\n",
        "\n",
        "      # Indices of next and previous transitions\n",
        "      prev_transition_index = idx - get_distance_to_prev_transition(df, idx)\n",
        "      next_transition_index = idx + get_distance_to_next_transition(df, idx)\n",
        "\n",
        "      # Define the start and end indices of the neighborhood\n",
        "      start_idx = idx - min(radius, max_prev_distance, max_next_distance)\n",
        "      end_idx = idx + min(radius, max_prev_distance, max_next_distance)\n",
        "\n",
        "      # Offset is where we define where we place the transition in the quartile, currently it is centered, rounding down\n",
        "      # We could add a little randomness here\n",
        "      neighborhood_size = end_idx - start_idx\n",
        "      offset = neighborhood_size // 3\n",
        "\n",
        "      # In cases with adjacent transitions this bonus allows us to grab at least one neighborhood where our transition is on the edge and we go out one radius length away from the other transition\n",
        "      if (max_next_distance == 0):\n",
        "        backward_bonus = radius\n",
        "      else :\n",
        "        backward_bonus = 0\n",
        "      if (max_prev_distance == 0):\n",
        "        forward_bonus = radius\n",
        "      else :\n",
        "        forward_bonus = 0\n",
        "      if (max_next_distance == 0 and max_prev_distance == 0):\n",
        "        continue\n",
        "\n",
        "      # Truncate this neighborhood if our desired offset captures another transition\n",
        "      if (prev_transition_index >= (start_idx - offset)):\n",
        "        last_third_start = prev_transition_index + 1\n",
        "      else :\n",
        "        last_third_start = start_idx - offset\n",
        "\n",
        "      if (next_transition_index <= (end_idx + offset + 1)):\n",
        "        first_third_end = next_transition_index - 1\n",
        "      else :\n",
        "        first_third_end = end_idx + offset\n",
        "\n",
        "      #It's not applying the backward bonus correctly\n",
        "\n",
        "      # Capture neighborhoods\n",
        "      first_third = df.iloc[start_idx + offset : first_third_end + 1 + forward_bonus]\n",
        "      mid_third = df.iloc[start_idx : end_idx + 1]\n",
        "      last_third = df.iloc[last_third_start - backward_bonus: end_idx - offset + 1]\n",
        "\n",
        "      # Make sure the transition isn't alone, then flatten\n",
        "      if (first_third[0] == 0).any():\n",
        "        # print_neighborhood_info(first_third, start_idx + offset, first_third_end + 1 + forward_bonus)\n",
        "        neighborhoods.append(flatten(first_third))\n",
        "\n",
        "      if (mid_third[0] == 0).any():\n",
        "        if not mid_third.equals(first_third):\n",
        "          # print_neighborhood_info(mid_third, start_idx, end_idx + 1)\n",
        "          neighborhoods.append(flatten(mid_third))\n",
        "\n",
        "      if (last_third[0] == 0).any():\n",
        "        if not last_third.equals(first_third) and not last_third.equals(mid_third):\n",
        "          # print_neighborhood_info(last_third, last_third_start - backward_bonus, end_idx - offset + 1)\n",
        "          neighborhoods.append(flatten(last_third))\n",
        "\n",
        "    return neighborhoods\n",
        "\n",
        "\n",
        "def get_distance_to_prev_transition(df, index) :\n",
        "  if index == 0:      # Start of story\n",
        "        return 1\n",
        "  for i in range(index - 1, -1, -1):\n",
        "      if df.iloc[i, 0] == 1:\n",
        "          return index - i\n",
        "  return None\n",
        "\n",
        "def get_distance_to_next_transition(df, index) :\n",
        "    for i in range(index + 1, len(df)):\n",
        "        if df.iloc[i, 0] == 1:\n",
        "            return i - index\n",
        "    return len(df) - index # Count the end of the story as the next scene transition\n",
        "\n",
        "def pad_neighborhood(neighborhood, length):\n",
        "    rows_to_add = length - len(neighborhood)\n",
        "\n",
        "    # If the DataFrame needs padding\n",
        "    if rows_to_add > 0:\n",
        "        padding = pd.DataFrame(np.nan, index=range(rows_to_add), columns=neighborhood.columns)\n",
        "        neighborhood = pd.concat([neighborhood, padding], ignore_index=True)\n",
        "\n",
        "    return neighborhood\n",
        "\n",
        "def flatten(neighborhood):\n",
        "    filtered_sentences = neighborhood[1].dropna().tolist()\n",
        "\n",
        "    # Join the sentences into a single string with spaces\n",
        "    concatenated_sentences = ' '.join(filtered_sentences)\n",
        "\n",
        "    # Create and return DataFrame with a single entry\n",
        "    return pd.DataFrame({'isTransition': [1], 'text': [concatenated_sentences]})\n"
      ],
      "metadata": {
        "id": "SlSPP_mjakIv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_control_sentence_neighborhoods(df, mean_radius, std_dev_radius, min_radius, max_radius):\n",
        "\n",
        "    non_transition_neighborhoods = []\n",
        "    transition_indices = set(df.index[df[0] == 1].tolist())\n",
        "    all_indices = set(range(len(df)))\n",
        "\n",
        "    # Non-transition indices are all indices minus the transition indices\n",
        "    non_transition_indices = list(all_indices - transition_indices)\n",
        "\n",
        "    for idx in non_transition_indices:\n",
        "        radius = generate_gaussian_window_size(mean_radius, std_dev_radius, min_radius, max_radius)\n",
        "        start_idx = max(idx - radius, 0)\n",
        "        end_idx = min(idx + radius, len(df) - 1)\n",
        "\n",
        "        if not set(range(start_idx, end_idx + 1)).intersection(transition_indices):\n",
        "            neighborhood = df.loc[start_idx:end_idx + 1].copy()\n",
        "\n",
        "            # This corrects for cases when the annotator left two sentences separated by one newline, by truncating the second\n",
        "            neighborhood.loc[:, 1] = neighborhood.loc[:, 1].apply(lambda x: x.split('\\n')[0])\n",
        "\n",
        "            if (neighborhood[0] == 0).all():  # Ensure no transitions within neighborhood\n",
        "                # print_neighborhood_info(neighborhood, start_idx, end_idx)\n",
        "                non_transition_neighborhoods.append(flatten_control(neighborhood.drop(columns=[0])))\n",
        "\n",
        "    return non_transition_neighborhoods\n"
      ],
      "metadata": {
        "id": "hTEpI5ITrPKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_neighborhoods = []\n",
        "\n",
        "for file_name, df in stories.items():\n",
        "    print(f\"Processing {file_name}...\")\n",
        "    try:\n",
        "        neighborhoods = generate_sentence_neighborhoods(df, MEAN, STD_DV, MIN_RADIUS, MAX_RADIUS)\n",
        "        all_neighborhoods.extend(neighborhoods)\n",
        "        print(f\"Added transition neighborhoods\")\n",
        "\n",
        "        control_neighborhoods = generate_control_sentence_neighborhoods(df, MEAN, STD_DV, MIN_RADIUS, MAX_RADIUS)\n",
        "\n",
        "      # If control_neighborhoods is larger, drop random entries but maintain order\n",
        "        while len(control_neighborhoods) > len(neighborhoods):\n",
        "            # Randomly select an index to remove\n",
        "            index_to_remove = random.randint(0, len(control_neighborhoods) - 1)\n",
        "            # Remove the entry at the selected index\n",
        "            del control_neighborhoods[index_to_remove]\n",
        "\n",
        "        all_neighborhoods.extend(control_neighborhoods)\n",
        "        print(f\"Added control neighborhoods\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {file_name}: {e}\")\n",
        "\n",
        "# Concatenate all neighborhood DataFrames into a single DataFrame\n",
        "if all_neighborhoods:\n",
        "    neighborhoods_df = pd.concat(all_neighborhoods, ignore_index=True)\n",
        "    neighborhoods_df.to_csv('mean=2_std=2_dataset.csv', index=False)\n",
        "    print(\"All neighborhoods have been exported to 'mean=2_std=2_dataset'.\")\n",
        "    print(neighborhoods_df)\n",
        "else:\n",
        "    print(\"No neighborhoods to export.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nOO1mgNMZ4oz",
        "outputId": "c2b2cda8-2476-428a-f62c-817750ef47c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing A_Christmas_Carol.xlsx...\n",
            "Added transition neighborhoods\n",
            "Added control neighborhoods\n",
            "Processing Collision_of_Worlds_1.xlsx...\n",
            "Added transition neighborhoods\n",
            "Added control neighborhoods\n",
            "Processing Collision_of_Worlds_2.xlsx...\n",
            "Added transition neighborhoods\n",
            "Added control neighborhoods\n",
            "Processing Collision_of_Worlds_3.xlsx...\n",
            "Added transition neighborhoods\n",
            "Added control neighborhoods\n",
            "Processing Collision_of_Worlds_4.xlsx...\n",
            "Added transition neighborhoods\n",
            "Added control neighborhoods\n",
            "Processing Collision_of_Worlds_5.xlsx...\n",
            "Added transition neighborhoods\n",
            "Added control neighborhoods\n",
            "Processing Falling_Sentences.xlsx...\n",
            "Added transition neighborhoods\n",
            "Added control neighborhoods\n",
            "Processing Frankenstein.xlsx...\n",
            "Added transition neighborhoods\n",
            "Added control neighborhoods\n",
            "Processing In_Amundsen’s_Tent.xlsx...\n",
            "Added transition neighborhoods\n",
            "Added control neighborhoods\n",
            "Processing Leiningen_Versus_the_Ants.xlsx...\n",
            "Added transition neighborhoods\n",
            "Added control neighborhoods\n",
            "Processing Observer_1-A_Warm_Home.xlsx...\n",
            "Added transition neighborhoods\n",
            "Added control neighborhoods\n",
            "Processing Observer_2-Charity.xlsx...\n",
            "Added transition neighborhoods\n",
            "Added control neighborhoods\n",
            "Processing Observer_3-One_of_Us.xlsx...\n",
            "Added transition neighborhoods\n",
            "Added control neighborhoods\n",
            "Processing Observer_4-Legends.xlsx...\n",
            "Added transition neighborhoods\n",
            "Added control neighborhoods\n",
            "Processing The_Demon_King.xlsx...\n",
            "Added transition neighborhoods\n",
            "Added control neighborhoods\n",
            "Processing The_Exiled_Queen.xlsx...\n",
            "Added transition neighborhoods\n",
            "Added control neighborhoods\n",
            "Processing The_Human_Chair.xlsx...\n",
            "Added transition neighborhoods\n",
            "Added control neighborhoods\n",
            "Processing The_Landlady.xlsx...\n",
            "Added transition neighborhoods\n",
            "Added control neighborhoods\n",
            "Processing The_Man_In_The_Well.xlsx...\n",
            "Added transition neighborhoods\n",
            "Added control neighborhoods\n",
            "Processing The_Monkey_s_Paw.xlsx...\n",
            "Added transition neighborhoods\n",
            "Added control neighborhoods\n",
            "Processing The_Most_Dangerous_Game.xlsx...\n",
            "Added transition neighborhoods\n",
            "Added control neighborhoods\n",
            "Processing The_Necklace.xlsx...\n",
            "Added transition neighborhoods\n",
            "Added control neighborhoods\n",
            "Processing The_Night_Rider.xlsx...\n",
            "Added transition neighborhoods\n",
            "Added control neighborhoods\n",
            "Processing The_Night_Wire.xlsx...\n",
            "Added transition neighborhoods\n",
            "Added control neighborhoods\n",
            "Processing The_Old_Man_And_The_Sea.xlsx...\n",
            "Added transition neighborhoods\n",
            "Added control neighborhoods\n",
            "Processing The_Ransom_of_Red_Chief.xlsx...\n",
            "Added transition neighborhoods\n",
            "Added control neighborhoods\n",
            "Processing The_Sun_Also_Rises.xlsx...\n",
            "Added transition neighborhoods\n",
            "Added control neighborhoods\n",
            "Processing To_Build_A_Fire.xlsx...\n",
            "Added transition neighborhoods\n",
            "Added control neighborhoods\n",
            "Processing Uncle_Toms_Cabin.xlsx...\n",
            "Added transition neighborhoods\n",
            "Added control neighborhoods\n",
            "Processing Winnie_The_Pooh.xlsx...\n",
            "Added transition neighborhoods\n",
            "Added control neighborhoods\n",
            "All neighborhoods have been exported to 'mean=2_std=2_dataset'.\n",
            "      isTransition                                               text\n",
            "0                1  Marley was dead: to begin with. There is no do...\n",
            "1                1  To edge his way along the crowded paths of lif...\n",
            "2                1  Scrooge resumed his labours with an improved o...\n",
            "3                1  Seeing clearly that it would be useless to pur...\n",
            "4                1  Good afternoon, gentlemen!” Seeing clearly tha...\n",
            "...            ...                                                ...\n",
            "8355             0  \"An Invitation!\" \"Yes, I heard you. Who droppe...\n",
            "8356             0  \"Ah!\" said Eeyore. \"A mistake, no doubt, but s...\n",
            "8357             0  But it didn't rain. Christopher Robin had made...\n",
            "8358             0  Roo jumped up and down in his seat for a littl...\n",
            "8359             0  Silly stuff. Nothing in it.\" Later on, when th...\n",
            "\n",
            "[8360 rows x 2 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def generate_token_neighborhoods(df, radius=TOKEN_NEIGHBORHOOD_RADIUS, token_length=TOKEN_LENGTH):\n",
        "  # My idea here is to grab the entire scene transition sentence, then instead of grabbing sentences around it, grab 4 token chunks backward and forward in the dataframe until we hit a partial word -- or maybe just keep the partial words\n",
        "\n",
        "    neighborhoods = []\n",
        "    length = ( radius * 2 ) + 1\n",
        "\n",
        "    transition_indices = df.index[df[0] == 1].tolist()  # Scene transition indices\n",
        "\n",
        "    for idx in transition_indices:\n",
        "\n",
        "      # These are the number of rows I can crawl up and down without hitting another transition\n",
        "      max_prev_distance = get_distance_to_prev_transition(df, idx) - 1\n",
        "      max_next_distance = get_distance_to_next_transition(df, idx) - 1\n",
        "\n",
        "      # Indices of next and previous transitions\n",
        "      prev_transition_index = idx - get_distance_to_prev_transition(df, idx)\n",
        "      next_transition_index = idx + get_distance_to_next_transition(df, idx)\n",
        "\n",
        "      # Define the start and end indices of the neighborhood\n",
        "      start_idx = idx - min(radius, max_prev_distance, max_next_distance)\n",
        "      end_idx = idx + min(radius, max_prev_distance, max_next_distance)\n",
        "\n",
        "      # Offset is where we define where we place the transition in the quartile, currently it is centered, rounding down\n",
        "      # We could add a little randomness here\n",
        "      neighborhood_size = end_idx - start_idx\n",
        "      offset = neighborhood_size // 3\n",
        "\n",
        "      # In cases with adjacent transitions this bonus allows us to grab at least one neighborhood where our transition is on the edge and we go out one radius length away from the other transition\n",
        "      if (max_next_distance == 0):\n",
        "        backward_bonus = radius\n",
        "      else :\n",
        "        backward_bonus = 0\n",
        "      if (max_prev_distance == 0):\n",
        "        forward_bonus = radius\n",
        "      else :\n",
        "        forward_bonus = 0\n",
        "      if (max_next_distance == 0 and max_prev_distance == 0):\n",
        "        continue\n",
        "\n",
        "      # Truncate this neighborhood if our desired offset captures another transition\n",
        "      if (prev_transition_index >= (start_idx - offset)):\n",
        "        last_third_start = prev_transition_index + 1\n",
        "      else :\n",
        "        last_third_start = start_idx - offset\n",
        "\n",
        "      if (next_transition_index <= (end_idx + offset + 1)):\n",
        "        first_third_end = next_transition_index - 1\n",
        "      else :\n",
        "        first_third_end = end_idx + offset\n",
        "\n",
        "      # Capture neighborhoods\n",
        "      first_third = df.iloc[start_idx + offset : first_third_end + 1 + forward_bonus]\n",
        "      mid_third = df.iloc[start_idx : end_idx + 1]\n",
        "      last_third = df.iloc[last_third_start - backward_bonus: end_idx - offset + 1]\n",
        "\n",
        "      # Make sure the transition isn't alone, then flatten\n",
        "      if (first_third[0] == 0).any():\n",
        "        neighborhoods.append(flatten(pad_neighborhood(first_third, length)))\n",
        "\n",
        "      if (mid_third[0] == 0).any():\n",
        "        neighborhoods.append(flatten(pad_neighborhood(mid_third, length)))\n",
        "\n",
        "      if (last_third[0] == 0).any():\n",
        "        neighborhoods.append(flatten(pad_neighborhood(last_third, length)))\n",
        "\n",
        "    return neighborhoods\n",
        "\n",
        "\n",
        "def get_distance_to_prev_transition(df, index) :\n",
        "  if index == 0:      # Start of story\n",
        "        return 1\n",
        "  for i in range(index - 1, -1, -1):\n",
        "      if df.iloc[i, 0] == 1:\n",
        "          return index - i\n",
        "  return None\n",
        "\n",
        "def get_distance_to_next_transition(df, index) :\n",
        "    for i in range(index + 1, len(df)):\n",
        "        if df.iloc[i, 0] == 1:\n",
        "            return i - index\n",
        "    return len(df) - index # Count the end of the story as the next scene transition\n",
        "\n",
        "def pad_neighborhood(neighborhood, length):\n",
        "    rows_to_add = length - len(neighborhood)\n",
        "\n",
        "    # If the DataFrame needs padding\n",
        "    if rows_to_add > 0:\n",
        "        padding = pd.DataFrame(np.nan, index=range(rows_to_add), columns=neighborhood.columns)\n",
        "        neighborhood = pd.concat([neighborhood, padding], ignore_index=True)\n",
        "\n",
        "    return neighborhood\n",
        "\n",
        "def flatten(neighborhood):\n",
        "    filtered_sentences = neighborhood[1].dropna().tolist()\n",
        "\n",
        "    # Join the sentences into a single string with spaces\n",
        "    concatenated_sentences = ' '.join(filtered_sentences)\n",
        "\n",
        "    # Create and return DataFrame with a single entry\n",
        "    return pd.DataFrame([concatenated_sentences])"
      ],
      "metadata": {
        "id": "c7g9vMOODzWk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
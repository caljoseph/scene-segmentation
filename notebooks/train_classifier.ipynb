{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/calebbradshaw/Library/Python/3.9/lib/python/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training data...\n",
      "Loading validation data...\n",
      "Loading test data...\n",
      "Epoch 1/200, Loss: 0.6242, Validation Loss: 0.5795, Validation Accuracy: 70.87%, Test Accuracy: 71.11%\n",
      "Epoch 2/200, Loss: 0.5687, Validation Loss: 0.5513, Validation Accuracy: 71.27%, Test Accuracy: 72.56%\n",
      "Epoch 3/200, Loss: 0.5559, Validation Loss: 0.5458, Validation Accuracy: 72.55%, Test Accuracy: 73.71%\n",
      "Epoch 4/200, Loss: 0.5383, Validation Loss: 0.5499, Validation Accuracy: 71.76%, Test Accuracy: 73.93%\n",
      "Epoch 5/200, Loss: 0.5302, Validation Loss: 0.5418, Validation Accuracy: 71.71%, Test Accuracy: 73.45%\n",
      "Epoch 6/200, Loss: 0.5230, Validation Loss: 0.5256, Validation Accuracy: 73.30%, Test Accuracy: 75.87%\n",
      "Epoch 7/200, Loss: 0.5058, Validation Loss: 0.5224, Validation Accuracy: 74.05%, Test Accuracy: 75.61%\n",
      "Epoch 8/200, Loss: 0.5038, Validation Loss: 0.5242, Validation Accuracy: 74.01%, Test Accuracy: 75.30%\n",
      "Epoch 9/200, Loss: 0.4971, Validation Loss: 0.5219, Validation Accuracy: 73.65%, Test Accuracy: 76.05%\n",
      "Epoch 10/200, Loss: 0.4845, Validation Loss: 0.5272, Validation Accuracy: 73.39%, Test Accuracy: 75.87%\n",
      "Epoch 11/200, Loss: 0.4755, Validation Loss: 0.5127, Validation Accuracy: 74.01%, Test Accuracy: 76.80%\n",
      "Epoch 12/200, Loss: 0.4711, Validation Loss: 0.5063, Validation Accuracy: 74.89%, Test Accuracy: 76.67%\n",
      "Epoch 13/200, Loss: 0.4715, Validation Loss: 0.5018, Validation Accuracy: 75.29%, Test Accuracy: 76.27%\n",
      "Epoch 14/200, Loss: 0.4638, Validation Loss: 0.5051, Validation Accuracy: 75.60%, Test Accuracy: 76.75%\n",
      "Epoch 15/200, Loss: 0.4495, Validation Loss: 0.5045, Validation Accuracy: 76.04%, Test Accuracy: 76.71%\n",
      "Epoch 16/200, Loss: 0.4429, Validation Loss: 0.4971, Validation Accuracy: 75.90%, Test Accuracy: 77.37%\n",
      "Epoch 17/200, Loss: 0.4381, Validation Loss: 0.4969, Validation Accuracy: 75.73%, Test Accuracy: 77.37%\n",
      "Epoch 18/200, Loss: 0.4390, Validation Loss: 0.4921, Validation Accuracy: 76.26%, Test Accuracy: 77.90%\n",
      "Epoch 19/200, Loss: 0.4242, Validation Loss: 0.4940, Validation Accuracy: 76.79%, Test Accuracy: 77.24%\n",
      "Epoch 20/200, Loss: 0.4201, Validation Loss: 0.5039, Validation Accuracy: 76.39%, Test Accuracy: 77.24%\n",
      "Epoch 21/200, Loss: 0.4228, Validation Loss: 0.4933, Validation Accuracy: 77.10%, Test Accuracy: 77.72%\n",
      "Epoch 22/200, Loss: 0.4151, Validation Loss: 0.4898, Validation Accuracy: 76.26%, Test Accuracy: 77.37%\n",
      "Epoch 23/200, Loss: 0.4095, Validation Loss: 0.5101, Validation Accuracy: 76.61%, Test Accuracy: 77.50%\n",
      "Epoch 24/200, Loss: 0.4034, Validation Loss: 0.5038, Validation Accuracy: 77.36%, Test Accuracy: 77.72%\n",
      "Epoch 25/200, Loss: 0.3895, Validation Loss: 0.4955, Validation Accuracy: 77.93%, Test Accuracy: 77.33%\n",
      "Epoch 26/200, Loss: 0.3843, Validation Loss: 0.4890, Validation Accuracy: 76.52%, Test Accuracy: 77.33%\n",
      "Epoch 27/200, Loss: 0.3911, Validation Loss: 0.4930, Validation Accuracy: 77.27%, Test Accuracy: 77.86%\n",
      "Epoch 28/200, Loss: 0.3788, Validation Loss: 0.5060, Validation Accuracy: 76.52%, Test Accuracy: 77.72%\n",
      "Epoch 29/200, Loss: 0.3776, Validation Loss: 0.5020, Validation Accuracy: 77.49%, Test Accuracy: 77.86%\n",
      "Epoch 30/200, Loss: 0.3803, Validation Loss: 0.5070, Validation Accuracy: 77.01%, Test Accuracy: 77.55%\n",
      "Epoch 31/200, Loss: 0.3652, Validation Loss: 0.4932, Validation Accuracy: 78.07%, Test Accuracy: 77.15%\n",
      "Epoch 32/200, Loss: 0.3719, Validation Loss: 0.4987, Validation Accuracy: 77.67%, Test Accuracy: 77.77%\n",
      "Epoch 33/200, Loss: 0.3646, Validation Loss: 0.4999, Validation Accuracy: 78.51%, Test Accuracy: 78.16%\n",
      "Epoch 34/200, Loss: 0.3546, Validation Loss: 0.5178, Validation Accuracy: 76.70%, Test Accuracy: 77.94%\n",
      "Epoch 35/200, Loss: 0.3649, Validation Loss: 0.5136, Validation Accuracy: 78.20%, Test Accuracy: 78.87%\n",
      "Epoch 36/200, Loss: 0.3642, Validation Loss: 0.5332, Validation Accuracy: 77.58%, Test Accuracy: 77.86%\n",
      "Epoch 37/200, Loss: 0.3533, Validation Loss: 0.5214, Validation Accuracy: 77.89%, Test Accuracy: 77.50%\n",
      "Epoch 38/200, Loss: 0.3520, Validation Loss: 0.5207, Validation Accuracy: 78.07%, Test Accuracy: 78.08%\n",
      "Epoch 39/200, Loss: 0.3494, Validation Loss: 0.5111, Validation Accuracy: 79.04%, Test Accuracy: 77.55%\n",
      "Epoch 40/200, Loss: 0.3422, Validation Loss: 0.5061, Validation Accuracy: 78.64%, Test Accuracy: 78.52%\n",
      "Epoch 41/200, Loss: 0.3525, Validation Loss: 0.5006, Validation Accuracy: 78.07%, Test Accuracy: 78.25%\n",
      "Epoch 42/200, Loss: 0.3423, Validation Loss: 0.5018, Validation Accuracy: 77.63%, Test Accuracy: 77.90%\n",
      "Epoch 43/200, Loss: 0.3423, Validation Loss: 0.5263, Validation Accuracy: 78.33%, Test Accuracy: 78.83%\n",
      "Epoch 44/200, Loss: 0.3332, Validation Loss: 0.5171, Validation Accuracy: 77.93%, Test Accuracy: 77.77%\n",
      "Epoch 45/200, Loss: 0.3353, Validation Loss: 0.5212, Validation Accuracy: 78.16%, Test Accuracy: 78.91%\n",
      "Epoch 46/200, Loss: 0.3319, Validation Loss: 0.5278, Validation Accuracy: 77.67%, Test Accuracy: 78.47%\n",
      "Epoch 47/200, Loss: 0.3199, Validation Loss: 0.5562, Validation Accuracy: 77.36%, Test Accuracy: 78.96%\n",
      "Epoch 48/200, Loss: 0.3180, Validation Loss: 0.5250, Validation Accuracy: 78.02%, Test Accuracy: 79.40%\n",
      "Epoch 49/200, Loss: 0.3264, Validation Loss: 0.5777, Validation Accuracy: 76.26%, Test Accuracy: 78.47%\n",
      "Epoch 50/200, Loss: 0.3242, Validation Loss: 0.5314, Validation Accuracy: 78.51%, Test Accuracy: 78.34%\n",
      "Epoch 51/200, Loss: 0.3261, Validation Loss: 0.5414, Validation Accuracy: 78.02%, Test Accuracy: 79.36%\n",
      "Epoch 52/200, Loss: 0.3225, Validation Loss: 0.5904, Validation Accuracy: 76.88%, Test Accuracy: 78.65%\n",
      "Epoch 53/200, Loss: 0.3238, Validation Loss: 0.5490, Validation Accuracy: 77.14%, Test Accuracy: 78.43%\n",
      "Epoch 54/200, Loss: 0.3170, Validation Loss: 0.5272, Validation Accuracy: 78.60%, Test Accuracy: 79.18%\n",
      "Epoch 55/200, Loss: 0.3101, Validation Loss: 0.5300, Validation Accuracy: 77.93%, Test Accuracy: 79.05%\n",
      "Epoch 56/200, Loss: 0.3113, Validation Loss: 0.5315, Validation Accuracy: 77.80%, Test Accuracy: 79.18%\n",
      "Epoch 57/200, Loss: 0.3102, Validation Loss: 0.5481, Validation Accuracy: 77.14%, Test Accuracy: 77.46%\n",
      "Epoch 58/200, Loss: 0.3124, Validation Loss: 0.5865, Validation Accuracy: 77.05%, Test Accuracy: 79.00%\n",
      "Epoch 59/200, Loss: 0.3083, Validation Loss: 0.5812, Validation Accuracy: 78.55%, Test Accuracy: 79.31%\n",
      "Epoch 60/200, Loss: 0.3086, Validation Loss: 0.5325, Validation Accuracy: 78.42%, Test Accuracy: 78.34%\n",
      "Epoch 61/200, Loss: 0.3046, Validation Loss: 0.5772, Validation Accuracy: 77.41%, Test Accuracy: 78.87%\n",
      "Epoch 62/200, Loss: 0.3045, Validation Loss: 0.5850, Validation Accuracy: 77.49%, Test Accuracy: 78.16%\n",
      "Epoch 63/200, Loss: 0.3056, Validation Loss: 0.5458, Validation Accuracy: 78.16%, Test Accuracy: 78.43%\n",
      "Epoch 64/200, Loss: 0.2947, Validation Loss: 0.5337, Validation Accuracy: 78.02%, Test Accuracy: 79.44%\n",
      "Epoch 65/200, Loss: 0.2895, Validation Loss: 0.5734, Validation Accuracy: 78.64%, Test Accuracy: 79.66%\n",
      "Epoch 66/200, Loss: 0.3021, Validation Loss: 0.5442, Validation Accuracy: 78.38%, Test Accuracy: 79.27%\n",
      "Epoch 67/200, Loss: 0.2999, Validation Loss: 0.5969, Validation Accuracy: 78.42%, Test Accuracy: 79.58%\n",
      "Epoch 68/200, Loss: 0.2957, Validation Loss: 0.5837, Validation Accuracy: 77.85%, Test Accuracy: 79.66%\n",
      "Epoch 69/200, Loss: 0.2890, Validation Loss: 0.5860, Validation Accuracy: 77.58%, Test Accuracy: 78.39%\n",
      "Epoch 70/200, Loss: 0.2929, Validation Loss: 0.6056, Validation Accuracy: 77.80%, Test Accuracy: 79.75%\n",
      "Epoch 71/200, Loss: 0.2902, Validation Loss: 0.5879, Validation Accuracy: 77.49%, Test Accuracy: 79.62%\n",
      "Epoch 72/200, Loss: 0.2810, Validation Loss: 0.6029, Validation Accuracy: 77.58%, Test Accuracy: 79.36%\n",
      "Epoch 73/200, Loss: 0.2834, Validation Loss: 0.6213, Validation Accuracy: 76.74%, Test Accuracy: 78.83%\n",
      "Epoch 74/200, Loss: 0.2915, Validation Loss: 0.6095, Validation Accuracy: 78.86%, Test Accuracy: 79.58%\n",
      "Epoch 75/200, Loss: 0.2765, Validation Loss: 0.5944, Validation Accuracy: 78.38%, Test Accuracy: 78.78%\n",
      "Epoch 76/200, Loss: 0.2787, Validation Loss: 0.6228, Validation Accuracy: 77.76%, Test Accuracy: 79.71%\n",
      "Epoch 77/200, Loss: 0.2832, Validation Loss: 0.6302, Validation Accuracy: 77.85%, Test Accuracy: 79.49%\n",
      "Epoch 78/200, Loss: 0.2858, Validation Loss: 0.6253, Validation Accuracy: 77.63%, Test Accuracy: 79.58%\n",
      "Epoch 79/200, Loss: 0.2837, Validation Loss: 0.6089, Validation Accuracy: 77.98%, Test Accuracy: 77.86%\n",
      "Epoch 80/200, Loss: 0.2813, Validation Loss: 0.6366, Validation Accuracy: 78.42%, Test Accuracy: 78.74%\n",
      "Epoch 81/200, Loss: 0.2800, Validation Loss: 0.6645, Validation Accuracy: 77.49%, Test Accuracy: 78.91%\n",
      "Epoch 82/200, Loss: 0.2830, Validation Loss: 0.6880, Validation Accuracy: 77.76%, Test Accuracy: 79.14%\n",
      "Epoch 83/200, Loss: 0.2682, Validation Loss: 0.7451, Validation Accuracy: 77.41%, Test Accuracy: 78.52%\n",
      "Epoch 84/200, Loss: 0.2718, Validation Loss: 0.6011, Validation Accuracy: 77.01%, Test Accuracy: 79.05%\n",
      "Epoch 85/200, Loss: 0.2664, Validation Loss: 0.6104, Validation Accuracy: 78.02%, Test Accuracy: 79.36%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[24], line 298\u001B[0m\n\u001B[1;32m    294\u001B[0m val_accuracies \u001B[38;5;241m=\u001B[39m []\n\u001B[1;32m    296\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(start_epoch, epochs):\n\u001B[1;32m    297\u001B[0m     \u001B[38;5;66;03m# Train for one epoch\u001B[39;00m\n\u001B[0;32m--> 298\u001B[0m     train_loss \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_epoch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mclassifier\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcriterion\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mscheduler\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    299\u001B[0m     train_losses\u001B[38;5;241m.\u001B[39mappend(train_loss)\n\u001B[1;32m    301\u001B[0m     \u001B[38;5;66;03m# Evaluate train and validation accuracy and loss\u001B[39;00m\n",
      "Cell \u001B[0;32mIn[24], line 147\u001B[0m, in \u001B[0;36mtrain_epoch\u001B[0;34m(classifier, optimizer, criterion, data_loader, device, scheduler)\u001B[0m\n\u001B[1;32m    145\u001B[0m predictions \u001B[38;5;241m=\u001B[39m classifier(embeddings)\n\u001B[1;32m    146\u001B[0m loss \u001B[38;5;241m=\u001B[39m criterion(predictions\u001B[38;5;241m.\u001B[39mview(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m), labels)\n\u001B[0;32m--> 147\u001B[0m \u001B[43mloss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    148\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mstep()\n\u001B[1;32m    149\u001B[0m total_loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m loss\u001B[38;5;241m.\u001B[39mitem()\n",
      "File \u001B[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/_tensor.py:525\u001B[0m, in \u001B[0;36mTensor.backward\u001B[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[1;32m    515\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    516\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[1;32m    517\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[1;32m    518\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    523\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[1;32m    524\u001B[0m     )\n\u001B[0;32m--> 525\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    526\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\n\u001B[1;32m    527\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/autograd/__init__.py:267\u001B[0m, in \u001B[0;36mbackward\u001B[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[1;32m    262\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[1;32m    264\u001B[0m \u001B[38;5;66;03m# The reason we repeat the same comment below is that\u001B[39;00m\n\u001B[1;32m    265\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[1;32m    266\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[0;32m--> 267\u001B[0m \u001B[43m_engine_run_backward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    268\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    269\u001B[0m \u001B[43m    \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    270\u001B[0m \u001B[43m    \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    271\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    272\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    273\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    274\u001B[0m \u001B[43m    \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    275\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/autograd/graph.py:744\u001B[0m, in \u001B[0;36m_engine_run_backward\u001B[0;34m(t_outputs, *args, **kwargs)\u001B[0m\n\u001B[1;32m    742\u001B[0m     unregister_hooks \u001B[38;5;241m=\u001B[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001B[1;32m    743\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 744\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mVariable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execution_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[1;32m    745\u001B[0m \u001B[43m        \u001B[49m\u001B[43mt_outputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\n\u001B[1;32m    746\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001B[39;00m\n\u001B[1;32m    747\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m    748\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m attach_logging_hooks:\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau, StepLR\n",
    "\n",
    "EMBEDDING_DIM = 384\n",
    "HIDDEN_DIM = 64\n",
    "\n",
    "# Function Definitions\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0]\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "class SimpleClassifier(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, output_dim, dropout_prob=0.7):\n",
    "        super(SimpleClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(embedding_dim, hidden_dim)\n",
    "        self.dropout1 = nn.Dropout(p=dropout_prob)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.dropout2 = nn.Dropout(p=dropout_prob)\n",
    "        self.fc3 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "class DeepClassifier(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, output_dim, dropout_prob=0.7):\n",
    "        super(DeepClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(embedding_dim, hidden_dim)\n",
    "        self.dropout1 = nn.Dropout(p=dropout_prob)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.dropout2 = nn.Dropout(p=dropout_prob)\n",
    "        self.fc3 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.dropout3 = nn.Dropout(p=dropout_prob)\n",
    "        self.fc4 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.dropout4 = nn.Dropout(p=dropout_prob)\n",
    "        self.fc5 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.dropout3(x)\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = self.dropout4(x)\n",
    "        x = self.fc5(x)\n",
    "        return x\n",
    "    \n",
    "class DeeperClassifier(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, output_dim, dropout_prob=0.7):\n",
    "        super(DeeperClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(embedding_dim, hidden_dim)\n",
    "        self.dropout1 = nn.Dropout(p=dropout_prob)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.dropout2 = nn.Dropout(p=dropout_prob)\n",
    "        self.fc3 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.dropout3 = nn.Dropout(p=dropout_prob)\n",
    "        self.fc4 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.dropout4 = nn.Dropout(p=dropout_prob)\n",
    "        self.fc5 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.dropout5 = nn.Dropout(p=dropout_prob)\n",
    "        self.fc6 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.dropout3(x)\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = self.dropout4(x)\n",
    "        x = F.relu(self.fc5(x))\n",
    "        x = self.dropout5(x)\n",
    "        x = self.fc6(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.fc1 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        out = F.relu(self.fc1(x))\n",
    "        out = self.fc2(out)\n",
    "        out += identity\n",
    "        return F.relu(out)\n",
    "\n",
    "class ResidualClassifier(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, output_dim, dropout_prob=0.7):\n",
    "        super(ResidualClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(embedding_dim, hidden_dim)\n",
    "        self.dropout1 = nn.Dropout(p=dropout_prob)\n",
    "        self.res_block1 = ResidualBlock(hidden_dim)\n",
    "        self.res_block2 = ResidualBlock(hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = self.res_block1(x)\n",
    "        x = self.res_block2(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "def encode_sentences_in_batches(sentences, tokenizer, model, device, batch_size=32):\n",
    "    embeddings = []\n",
    "    for i in range(0, len(sentences), batch_size):\n",
    "        batch = sentences[i:i+batch_size]\n",
    "        encoded_input = tokenizer(batch, padding=True, truncation=True, return_tensors='pt').to(device)\n",
    "        with torch.no_grad():\n",
    "            model_output = model(**encoded_input)\n",
    "        batch_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "        embeddings.append(F.normalize(batch_embeddings, p=2, dim=1).cpu().numpy())\n",
    "    return np.vstack(embeddings)\n",
    "\n",
    "def train_epoch(classifier, optimizer, criterion, data_loader, device, scheduler=None):\n",
    "    classifier.train()\n",
    "    total_loss = 0\n",
    "    for embeddings, labels in data_loader:\n",
    "        embeddings = embeddings.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        predictions = classifier(embeddings)\n",
    "        loss = criterion(predictions.view(-1), labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    # if scheduler:\n",
    "        # scheduler.step()\n",
    "\n",
    "    return total_loss / len(data_loader)\n",
    "\n",
    "def evaluate_accuracy(classifier, data_loader, device):\n",
    "    classifier.eval()\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for embeddings, labels in data_loader:\n",
    "            embeddings = embeddings.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            predictions = classifier(embeddings)\n",
    "            predicted_labels = predictions.view(-1) > 0.0\n",
    "            correct += (predicted_labels == labels.byte()).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    return correct / total\n",
    "\n",
    "def prepare_data_loader(embeddings, labels, batch_size=16):\n",
    "    dataset = TensorDataset(torch.tensor(embeddings, dtype=torch.float32), torch.tensor(labels, dtype=torch.float32))\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "def evaluate_loss(classifier, data_loader, criterion, device):\n",
    "    classifier.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for embeddings, labels in data_loader:\n",
    "            embeddings = embeddings.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            predictions = classifier(embeddings)\n",
    "            loss = criterion(predictions.view(-1), labels)\n",
    "            total_loss += loss.item()\n",
    "    return total_loss / len(data_loader)\n",
    "\n",
    "# Set device\n",
    "device = \"mps\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv('example_dataset_with_controls.csv')\n",
    "train_data, temp_data = train_test_split(df, test_size=0.4, random_state=42)\n",
    "val_data, test_data = train_test_split(temp_data, test_size=0.5, random_state=42)\n",
    "\n",
    "# Prepare tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\n",
    "model = AutoModel.from_pretrained('sentence-transformers/all-MiniLM-L6-v2').to(device)\n",
    "\n",
    "# Paths to save embeddings\n",
    "train_embeddings_file = 'train_embeddings.npy'\n",
    "val_embeddings_file = 'val_embeddings.npy'\n",
    "test_embeddings_file = 'test_embeddings.npy'\n",
    "train_labels_file = 'train_labels.npy'\n",
    "val_labels_file = 'val_labels.npy'\n",
    "test_labels_file = 'test_labels.npy'\n",
    "\n",
    "# Encode sentences and save embeddings if not already saved\n",
    "def save_embeddings_and_labels(embeddings_file, labels_file, data, tokenizer, model, device, batch_size=32):\n",
    "    sentences = data['text'].tolist()\n",
    "    labels = data['isTransition'].values\n",
    "    embeddings = encode_sentences_in_batches(sentences, tokenizer, model, device, batch_size)\n",
    "    np.save(embeddings_file, embeddings)\n",
    "    np.save(labels_file, labels)\n",
    "    return embeddings, labels\n",
    "\n",
    "def load_embeddings_and_labels(embeddings_file, labels_file):\n",
    "    embeddings = np.load(embeddings_file)\n",
    "    labels = np.load(labels_file)\n",
    "    return embeddings, labels\n",
    "\n",
    "if not os.path.exists(train_embeddings_file) or not os.path.exists(train_labels_file):\n",
    "    print(\"Encoding and saving training data...\")\n",
    "    train_embeddings, train_labels = save_embeddings_and_labels(train_embeddings_file, train_labels_file, train_data, tokenizer, model, device)\n",
    "else:\n",
    "    print(\"Loading training data...\")\n",
    "    train_embeddings, train_labels = load_embeddings_and_labels(train_embeddings_file, train_labels_file)\n",
    "\n",
    "if not os.path.exists(val_embeddings_file) or not os.path.exists(val_labels_file):\n",
    "    print(\"Encoding and saving validation data...\")\n",
    "    val_embeddings, val_labels = save_embeddings_and_labels(val_embeddings_file, val_labels_file, val_data, tokenizer, model, device)\n",
    "else:\n",
    "    print(\"Loading validation data...\")\n",
    "    val_embeddings, val_labels = load_embeddings_and_labels(val_embeddings_file, val_labels_file)\n",
    "\n",
    "if not os.path.exists(test_embeddings_file) or not os.path.exists(test_labels_file):\n",
    "    print(\"Encoding and saving test data...\")\n",
    "    test_embeddings, test_labels = save_embeddings_and_labels(test_embeddings_file, test_labels_file, test_data, tokenizer, model, device)\n",
    "else:\n",
    "    print(\"Loading test data...\")\n",
    "    test_embeddings, test_labels = load_embeddings_and_labels(test_embeddings_file, test_labels_file)\n",
    "\n",
    "# Prepare DataLoaders\n",
    "train_loader = prepare_data_loader(train_embeddings, train_labels)\n",
    "val_loader = prepare_data_loader(val_embeddings, val_labels)\n",
    "test_loader = prepare_data_loader(test_embeddings, test_labels)\n",
    "\n",
    "# Initialize classifier, optimizer, and loss function\n",
    "\n",
    "# Select model architecture\n",
    "model_type = input(\"Enter model type (simple, deep, deeper, residual): \").strip().lower()\n",
    "\n",
    "if model_type == \"simple\":\n",
    "    classifier = SimpleClassifier(embedding_dim=384, hidden_dim=HIDDEN_DIM, output_dim=1, dropout_prob=0.7).to(device)\n",
    "elif model_type == \"deep\":\n",
    "    classifier = DeepClassifier(embedding_dim=384, hidden_dim=HIDDEN_DIM, output_dim=1, dropout_prob=0.7).to(device)\n",
    "elif model_type == \"residual\":\n",
    "    classifier = ResidualClassifier(embedding_dim=384, hidden_dim=HIDDEN_DIM, output_dim=1, dropout_prob=0.7).to(device)\n",
    "elif model_type == \"deeper\":\n",
    "    classifier = DeeperClassifier(embedding_dim=384, hidden_dim=HIDDEN_DIM, output_dim=1, dropout_prob=0.7).to(device)\n",
    "else:\n",
    "    print(\"Invalid model type. Defaulting to simple model.\")\n",
    "    classifier = SimpleClassifier(embedding_dim=384, hidden_dim=HIDDEN_DIM, output_dim=1, dropout_prob=0.7).to(device)\n",
    "    \n",
    "optimizer = optim.Adam(classifier.parameters(), lr=1e-3)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# Set up learning rate scheduler\n",
    "# scheduler = ReduceLROnPlateau(optimizer, 'min', patience=5)\n",
    "# scheduler = StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "\n",
    "# Model save path\n",
    "model_path = 'classifier_state_768_dims.pth'\n",
    "\n",
    "# Check if a saved state exists and load it\n",
    "if os.path.exists(model_path):\n",
    "    print(\"Loading checkpoint...\")\n",
    "    checkpoint = torch.load(model_path)\n",
    "    classifier.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    start_epoch = checkpoint['epoch'] + 1  # Continue from next epoch\n",
    "else:\n",
    "    start_epoch = 0  # Start from scratch\n",
    "\n",
    "# Training Loop\n",
    "epochs = int(input(\"Enter the number of epochs to train for: \"))\n",
    "max_accuracy = 0\n",
    "stopper_count = 0\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_accuracies = []\n",
    "val_accuracies = []\n",
    "\n",
    "for epoch in range(start_epoch, epochs):\n",
    "    # Train for one epoch\n",
    "    train_loss = train_epoch(classifier, optimizer, criterion, train_loader, device, scheduler)\n",
    "    train_losses.append(train_loss)\n",
    "\n",
    "    # Evaluate train and validation accuracy and loss\n",
    "    train_accuracy = evaluate_accuracy(classifier, train_loader, device)\n",
    "    val_loss = evaluate_loss(classifier, val_loader, criterion, device)\n",
    "    val_accuracy = evaluate_accuracy(classifier, val_loader, device)\n",
    "    train_accuracies.append(train_accuracy)\n",
    "    val_losses.append(val_loss)\n",
    "    val_accuracies.append(val_accuracy)\n",
    "\n",
    "    # scheduler.step(val_loss)\n",
    "\n",
    "    # Evaluate accuracy on test set\n",
    "    test_accuracy = evaluate_accuracy(classifier, test_loader, device)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy*100:.2f}%, Test Accuracy: {test_accuracy*100:.2f}%\")\n",
    "\n",
    "    # Save the model state if best accuracy so far\n",
    "    if test_accuracy > max_accuracy:\n",
    "        stopper_count = 0  # Reset stopping criteria\n",
    "        max_accuracy = test_accuracy\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': classifier.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "        }, model_path)\n",
    "    else:\n",
    "        stopper_count += 1  # Increment stopping criteria\n",
    "\n",
    "    # Early stopping criteria\n",
    "    if stopper_count == 50:\n",
    "        print(\"Early stopping criteria met. Stopping training.\")\n",
    "        break  # If 10 epochs pass without improvement, end training.\n",
    "\n",
    "# Plot the train and validation accuracies and losses\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_losses, label='Training Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(train_accuracies, label='Training Accuracy')\n",
    "plt.plot(val_accuracies, label='Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Evaluate on test set\n",
    "test_accuracy = evaluate_accuracy(classifier, test_loader, device)\n",
    "print(f\"Test Accuracy: {test_accuracy*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
